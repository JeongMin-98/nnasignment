import os
import itertools
import multiprocessing
import time
import argparse

# ====== ì‹¤í—˜ ì„¤ì • ======
models = ['resnet18','resnet34','resnet50']
optimizers = ['sgd', 'adam']
batch_size = 32
epochs = 20
lr = 0.001
use_wandb = True
max_concurrent = 3
max_retries = 1

# ====== ê²½ë¡œ ì„¤ì • ======
log_dir = "log"
status_dir = "status"
success_log_path = "success_runs.txt"
failed_log_path = "failed_runs.txt"
os.makedirs(log_dir, exist_ok=True)
os.makedirs(status_dir, exist_ok=True)

# ====== ì„¸ë§ˆí¬ì–´ & ë½ ======
semaphore = multiprocessing.Semaphore(max_concurrent)
lock = multiprocessing.Lock()

def write_status(run_id: str, status: str):
    with open(os.path.join(status_dir, f"{run_id}.status"), "w") as f:
        f.write(status)

def run_and_retry(model: str, optimizer: str):
    run_id = f"{model}-{optimizer}"
    log_path = os.path.join(log_dir, f"{run_id}.txt")
    attempt = 0
    success = False

    while attempt < max_retries and not success:
        attempt += 1
        write_status(run_id, f"running (attempt {attempt})")
        print(f"ğŸš€ [{run_id}] ì‹œë„ {attempt} ì‹œì‘")

        with semaphore:
            cmd = f"python main.py --model {model} --optimizer {optimizer} --batch_size {batch_size} --epochs {epochs} --lr {lr}"
            if use_wandb:
                cmd += " --use_wandb"
            full_cmd = f"{cmd} > {log_path} 2>&1"
            result = os.system(full_cmd)

        if result == 0:
            success = True
            write_status(run_id, "success")
            with lock:
                with open(success_log_path, 'a') as sf:
                    sf.write(f"{run_id}\n")
            print(f"âœ… [{run_id}] ì„±ê³µ")
        else:
            write_status(run_id, f"fail (attempt {attempt})")
            print(f"âŒ [{run_id}] ì‹¤íŒ¨ (ì‹œë„ {attempt})")
            if attempt < max_retries:
                time.sleep(5)

    if not success:
        with lock:
            with open(failed_log_path, 'a') as ff:
                ff.write(f"{run_id}\n")
        write_status(run_id, f"failed after {max_retries} attempts")

def monitor_status(total_runs: int):
    while True:
        status_files = sorted(os.listdir(status_dir))
        os.system('clear' if os.name != 'nt' else 'cls')
        print("ğŸ“¡ ì‹¤í—˜ ìƒíƒœ ëª¨ë‹ˆí„°ë§ ì¤‘...\n")
        summary = {"running": 0, "success": 0, "fail": 0}
        for fname in status_files:
            with open(os.path.join(status_dir, fname)) as f:
                content = f.read().strip()
                run_id = fname.replace('.status', '')
                print(f"ğŸ”¹ {run_id}: {content}")
                if "success" in content:
                    summary["success"] += 1
                elif "fail" in content:
                    summary["fail"] += 1
                elif "running" in content:
                    summary["running"] += 1
        print(f"\nğŸ“Š ìš”ì•½: âœ… {summary['success']}  âŒ {summary['fail']}  ğŸ”„ {summary['running']}")
        if summary["success"] + summary["fail"] >= total_runs:
            print("\nğŸ§© ëª¨ë“  ì‹¤í—˜ ì™„ë£Œ")
            break
        time.sleep(3)

def load_completed_runs() -> set:
    completed = set()
    if os.path.exists(success_log_path):
        with open(success_log_path) as f:
            completed.update([line.strip() for line in f])
    return completed

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--resume', action='store_true', help='ì´ë¯¸ ì„±ê³µí•œ ì‹¤í—˜ì€ ê±´ë„ˆë›°ê¸°')
    args = parser.parse_args()

    # ì‹¤í—˜ ëª©ë¡ ìƒì„±
    run_combinations = list(itertools.product(models, optimizers))
    total_runs = len(run_combinations)

    # ì´ì „ ìƒíƒœ ìœ ì§€ or ì´ˆê¸°í™”
    if not args.resume:
        for path in [success_log_path, failed_log_path]:
            if os.path.exists(path):
                os.remove(path)
        for f in os.listdir(status_dir):
            os.remove(os.path.join(status_dir, f))
        completed_runs = set()
    else:
        completed_runs = load_completed_runs()
        print(f"ğŸ” ì¬ì‹œì‘ ëª¨ë“œ: {len(completed_runs)}ê°œëŠ” ìƒëµë¨")

    # ì‹¤í—˜ ì¡°í•© í•„í„°ë§
    jobs = []
    filtered_runs = []
    for model, optimizer in run_combinations:
        run_id = f"{model}-{optimizer}"
        if run_id in completed_runs:
            continue
        filtered_runs.append((model, optimizer))

    # ëª¨ë‹ˆí„° í”„ë¡œì„¸ìŠ¤
    monitor_proc = multiprocessing.Process(target=monitor_status, args=(len(filtered_runs),))
    monitor_proc.start()

    # ë³‘ë ¬ ì‹¤í—˜ ì‹¤í–‰
    for model, optimizer in filtered_runs:
        p = multiprocessing.Process(target=run_and_retry, args=(model, optimizer))
        jobs.append(p)
        p.start()

    for job in jobs:
        job.join()
    monitor_proc.join()

    print("\nâœ… ì „ì²´ ì‹¤í—˜ ì¢…ë£Œ ì™„ë£Œ")
